{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Images\n",
    "Here, we will be defining the function which will load all the images from the generated dataset and return the images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    shapes = ['line', 'circle', 'ellipse', 'rectangle', 'rounded_rectangle', 'regular_polygon', 'star']\n",
    "    \n",
    "    for shape in shapes:\n",
    "        shape_dir = os.path.join(base_dir, shape)\n",
    "        for filename in os.listdir(shape_dir):\n",
    "            img_path = os.path.join(shape_dir, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if(img is not None):\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                data.append(img)\n",
    "                labels.append(shape)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now go ahead and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../dataset'  # Path to the generated dataset\n",
    "data, labels = load_images_from_directory(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "Here, we will be defining the function which will preprocess the data. We will be normalizing the images and one-hot encoding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,labels):\n",
    "    data = data.reshape((data.shape[0], 128, 128, 1)) / 255.0 # Normalize the data\n",
    "    label_encoder = LabelEncoder() # Encode the labels\n",
    "    labels = to_categorical(label_encoder.fit_transform(labels)) # One-hot encode the labels\n",
    "    return data,labels,label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y,label_encoder = preprocess(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model architecture using CNN\n",
    "\n",
    "## Layer-wise Breakdown:\n",
    "\n",
    "**Input Layer (`input_shape=(128, 128, 1)`)**\n",
    "\n",
    "* This defines the shape of the images the model will receive: 128 pixels wide, 128 pixels high, and 1 channel (grayscale).\n",
    "\n",
    "**Convolutional Layers (`Conv2D`)**\n",
    "\n",
    "* We have 5 convolutional layers with increasing numbers of filters (32, 64, 128, 256, 512).\n",
    "* Each layer uses a 3x3 filter size (`(3, 3)`) to detect local patterns in the image.\n",
    "* The `ReLU` activation function introduces non-linearity, allowing the model to learn complex relationships between features.\n",
    "\n",
    "**Max Pooling Layers (`MaxPooling2D`)**\n",
    "\n",
    "* Each convolutional layer is followed by a max-pooling layer with a pool size of 2x2 (`(2, 2)`).\n",
    "* Max pooling reduces the spatial dimensions of the feature maps, helping to make the model more robust to small shifts and variations in the shapes' positions.\n",
    "\n",
    "**Flatten Layer (`Flatten`)**\n",
    "\n",
    "* This layer converts the multidimensional output of the final convolutional layer into a single-dimensional vector, preparing it for the fully connected layers.\n",
    "\n",
    "**Dense Layers (`Dense`)**\n",
    "\n",
    "* The first dense layer has 512 neurons and uses the `ReLU` activation function. It performs high-level feature extraction and pattern recognition.\n",
    "* The final dense layer has `len(label_encoder.classes_)` neurons (the number of shape classes) and uses the `softmax` activation function. Softmax outputs probabilities for each class, allowing us to interpret the model's prediction as the class with the highest probability.\n",
    "\n",
    "**Dropout Layer (`Dropout(0.8)`)**\n",
    "\n",
    "* This layer randomly drops out (sets to zero) 80% of the neurons during training.\n",
    "* Dropout acts as a regularization technique, preventing overfitting by forcing the network to learn more robust features and reducing reliance on any single neuron.\n",
    "\n",
    "## Justification for 0.8 Dropout\n",
    "\n",
    "* **Addressing Overfitting:** The relatively high dropout rate of 0.8 is likely chosen to combat overfitting, which can be a concern when training deep convolutional neural networks.\n",
    "* **Empirical Choice:** The specific value of 0.8 might have been determined through experimentation and validation on your dataset. It's possible that lower dropout rates were tried but resulted in worse performance on the validation or test set.\n",
    "\n",
    "## Overall Model Design Philosophy\n",
    "\n",
    "The model follows a common pattern in image classification tasks:\n",
    "\n",
    "1. **Feature Extraction:** The convolutional and pooling layers extract increasingly complex features from the images, starting with simple edges and corners and progressing to more abstract representations of shapes.\n",
    "2. **Classification:** The dense layers, along with the softmax activation, perform the final classification based on the extracted features.\n",
    "3. **Regularization:** Dropout helps prevent overfitting and improve the model's generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/Desktop/Adobe_GenSolve/CURVETOPIA_DA/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(512, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.8),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 532ms/step - accuracy: 0.2333 - loss: 1.8266 - val_accuracy: 0.5227 - val_loss: 1.0968\n",
      "Epoch 2/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 552ms/step - accuracy: 0.4911 - loss: 1.1807 - val_accuracy: 0.6504 - val_loss: 0.6573\n",
      "Epoch 3/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 539ms/step - accuracy: 0.6371 - loss: 0.7611 - val_accuracy: 0.6908 - val_loss: 0.6122\n",
      "Epoch 4/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 512ms/step - accuracy: 0.7166 - loss: 0.5925 - val_accuracy: 0.7706 - val_loss: 0.4255\n",
      "Epoch 5/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 508ms/step - accuracy: 0.7452 - loss: 0.4795 - val_accuracy: 0.7714 - val_loss: 0.4167\n",
      "Epoch 6/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 504ms/step - accuracy: 0.7741 - loss: 0.4302 - val_accuracy: 0.7950 - val_loss: 0.3778\n",
      "Epoch 7/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 536ms/step - accuracy: 0.7949 - loss: 0.3930 - val_accuracy: 0.8092 - val_loss: 0.3410\n",
      "Epoch 8/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 540ms/step - accuracy: 0.7937 - loss: 0.3701 - val_accuracy: 0.8059 - val_loss: 0.3475\n",
      "Epoch 9/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 546ms/step - accuracy: 0.8241 - loss: 0.3240 - val_accuracy: 0.8034 - val_loss: 0.3280\n",
      "Epoch 10/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 512ms/step - accuracy: 0.8089 - loss: 0.3419 - val_accuracy: 0.7958 - val_loss: 0.3236\n",
      "Epoch 11/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 496ms/step - accuracy: 0.8242 - loss: 0.3049 - val_accuracy: 0.8034 - val_loss: 0.3757\n",
      "Epoch 12/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 545ms/step - accuracy: 0.8397 - loss: 0.2825 - val_accuracy: 0.8050 - val_loss: 0.3515\n",
      "Epoch 13/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 525ms/step - accuracy: 0.8363 - loss: 0.2783 - val_accuracy: 0.8084 - val_loss: 0.3271\n",
      "Epoch 14/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 500ms/step - accuracy: 0.8411 - loss: 0.2628 - val_accuracy: 0.7992 - val_loss: 0.3351\n",
      "Epoch 15/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 500ms/step - accuracy: 0.8417 - loss: 0.2757 - val_accuracy: 0.7857 - val_loss: 0.3920\n",
      "Epoch 16/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 496ms/step - accuracy: 0.8502 - loss: 0.2655 - val_accuracy: 0.7983 - val_loss: 0.3724\n",
      "Epoch 17/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 495ms/step - accuracy: 0.8345 - loss: 0.2688 - val_accuracy: 0.8227 - val_loss: 0.3040\n",
      "Epoch 18/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 523ms/step - accuracy: 0.8566 - loss: 0.2344 - val_accuracy: 0.8034 - val_loss: 0.3240\n",
      "Epoch 19/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 497ms/step - accuracy: 0.8429 - loss: 0.2703 - val_accuracy: 0.8235 - val_loss: 0.3757\n",
      "Epoch 20/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 501ms/step - accuracy: 0.8495 - loss: 0.2507 - val_accuracy: 0.7966 - val_loss: 0.3522\n",
      "Epoch 21/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 494ms/step - accuracy: 0.8596 - loss: 0.2355 - val_accuracy: 0.8025 - val_loss: 0.3522\n",
      "Epoch 22/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 497ms/step - accuracy: 0.8348 - loss: 0.2832 - val_accuracy: 0.7966 - val_loss: 0.3352\n",
      "Epoch 23/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 502ms/step - accuracy: 0.8688 - loss: 0.2218 - val_accuracy: 0.8143 - val_loss: 0.3851\n",
      "Epoch 24/25\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 493ms/step - accuracy: 0.8688 - loss: 0.2120 - val_accuracy: 0.8059 - val_loss: 0.3706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x37af08e90>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=25, validation_split=0.2, batch_size=32,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - accuracy: 0.8359 - loss: 0.2958\n",
      "Accuracy on test data: 84.67%\n",
      "Loss on test data: 29.13%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n",
    "print(f\"Loss on test data: {loss * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation and Rationale:\n",
    "\n",
    "* Test Accuracy of 84.67%: This accuracy indicates the model's strong capability in correctly classifying unseen shape images, showcasing its effective generalization to new data.\n",
    "\n",
    "* Loss of 29.13%: While there is room for further improvement, this loss value suggests the model's predictions are reasonably aligned with the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('../models/shapes_created_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
